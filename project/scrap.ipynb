{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best: 0.800507 using {'batch_size': 10, 'epochs': 20, 'n_hidden': 2, 'nodes': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual\n",
    "\n",
    "def create_keras_model(input_dim=11, output_dim=5, n_hidden=4, name='model'):\n",
    "    '''\n",
    "    How to call:\n",
    "    myModel = create_keras_model(n_features, n_classes, nodes=5, n=4, name='model')\n",
    "    '''\n",
    "\n",
    "    # define input layer\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "\n",
    "    # list to hold all the hidden layers\n",
    "    hidden_layers = []\n",
    "\n",
    "    # create the hidden layers\n",
    "\n",
    "    # if it's the first hidden layer, connect it to the inputs\n",
    "    hidden_layers.append(Dense(units=6, activation='relu', kernel_initializer='random_normal')(inputs))\n",
    "\n",
    "    # otherwise, connect it to the previous hidden layer\n",
    "    hidden_layers.append(Dense(units=5, activation='relu', kernel_initializer='random_normal')(hidden_layers[-1]))\n",
    "            \n",
    "    # create the final hidden layer, connected to all previous hidden layers\n",
    "    final_hidden = Dense(units=nodes, activation='relu', kernel_initializer='random_normal')(concatenate(hidden_layers))\n",
    "\n",
    "    # create output layer connected to the final hidden layer\n",
    "    outputs = Dense(output_dim, activation='softmax')(final_hidden)\n",
    "\n",
    "    # create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model0 = create_keras_model(n_features, n_classes, nodes=5, n_hidden=4, name='model')\n",
    "model0.summary()\n",
    "plot_model(model0, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 21, 41]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1, 50, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_keras_model(input_dim, output_dim, nodes=5, n_hidden=1, name='model'):\n",
    "#     '''\n",
    "#     How to call:\n",
    "#     myModel = create_keras_model(n_features, n_classes, nodes=5, n=1, name='model')\n",
    "#     '''\n",
    "#     # Create model\n",
    "#     model = Sequential(name=name)\n",
    "#     model.add(Dense(units=nodes,\n",
    "#                     input_dim=input_dim,\n",
    "#                     activation='relu',\n",
    "#                     kernel_initializer='random_normal'\n",
    "#                    )\n",
    "#              )\n",
    "#     # add HIDDEN layers to the network:\n",
    "#     for i in range(n_hidden):\n",
    "#         model.add(Dense(units=nodes,\n",
    "#                         activation='relu',\n",
    "#                         kernel_initializer='random_normal'\n",
    "#                        )\n",
    "#                  )\n",
    "#     # OUTPUT layer:\n",
    "#     model.add(Dense(output_dim, \n",
    "#                     activation='softmax', #  sigmoid\n",
    "#                    )\n",
    "#              )\n",
    "    \n",
    "#     model.compile(optimizer='adam', #  sgd\n",
    "#                   loss='categorical_crossentropy',\n",
    "#                   metrics=['accuracy']\n",
    "#                  )\n",
    "    \n",
    "#     return model\n",
    "\n",
    "\n",
    "# def create_keras_model(input_dim, output_dim, nodes=5, n_hidden=4, name='model'):\n",
    "#     '''\n",
    "#     How to call:\n",
    "#     myModel = create_keras_model(n_features, n_classes, nodes=5, n=4, name='model')\n",
    "#     '''\n",
    "\n",
    "#     # define input layer\n",
    "#     inputs = Input(shape=(input_dim,))\n",
    "\n",
    "#     # list to hold all the hidden layers\n",
    "#     hidden_layers = []\n",
    "\n",
    "#     # create the hidden layers\n",
    "#     for i in range(n_hidden):\n",
    "#         if i == 0:\n",
    "#             # if it's the first hidden layer, connect it to the inputs\n",
    "#             hidden_layers.append(Dense(units=nodes, activation='relu', kernel_initializer='random_normal')(inputs))\n",
    "#         else:\n",
    "#             # otherwise, connect it to the previous hidden layer\n",
    "#             hidden_layers.append(Dense(units=nodes, activation='relu', kernel_initializer='random_normal')(hidden_layers[-1]))\n",
    "            \n",
    "#     # create the final hidden layer, connected to all previous hidden layers\n",
    "#     final_hidden = Dense(units=nodes, activation='relu', kernel_initializer='random_normal')(concatenate(hidden_layers))\n",
    "\n",
    "#     # create output layer connected to the final hidden layer\n",
    "#     outputs = Dense(output_dim, activation='softmax')(final_hidden)\n",
    "\n",
    "#     # create model\n",
    "#     model = Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     return model\n",
    "\n",
    "# def create_keras_model(input_dim=11, output_dim=5, nodes=5, n_hidden=4, name='model'):\n",
    "#     '''\n",
    "#     How to call:\n",
    "#     myModel = create_keras_model(n_features, n_classes, nodes=5, n=4, name='model')\n",
    "#     '''\n",
    "\n",
    "#     # define input layer\n",
    "#     inputs = Input(shape=(input_dim,))\n",
    "\n",
    "#     # list to hold all the hidden layers\n",
    "#     hidden_layers = []\n",
    "\n",
    "#     # create the hidden layers\n",
    "#     for i in range(n_hidden-1):\n",
    "#         if i == 0:\n",
    "#             # if it's the first hidden layer, connect it to the inputs\n",
    "#             hidden_layers.append(Dense(units=nodes, activation='relu', kernel_initializer='random_normal')(inputs))\n",
    "#         else:\n",
    "#             # otherwise, connect it to the previous hidden layer\n",
    "#             hidden_layers.append(Dense(units=nodes, activation='relu', kernel_initializer='random_normal')(hidden_layers[-1]))\n",
    "            \n",
    "#     # create the final hidden layer, connected to all previous hidden layers\n",
    "#     final_hidden = Dense(units=nodes, activation='relu', kernel_initializer='random_normal')(concatenate(hidden_layers))\n",
    "\n",
    "#     # create output layer connected to the final hidden layer\n",
    "#     outputs = Dense(output_dim, activation='softmax')(final_hidden)\n",
    "\n",
    "#     # create model\n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model0.metrics_names\n",
    "\n",
    "\n",
    "# score = model0.evaluate(X_test, Y_test, verbose=1)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(2, figsize=(9, 8))\n",
    "\n",
    "# val_accurady = model0firstfit.history['val_accuracy']\n",
    "# val_loss = model0firstfit.history['val_loss']\n",
    "# ax1.plot(val_accurady)\n",
    "# ax2.plot(val_loss)\n",
    "    \n",
    "# ax1.set_ylabel('Validation Accuracy')\n",
    "# ax2.set_ylabel('Validation Loss')\n",
    "# ax2.set_xlabel('epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_keras_model(input_dim=11, output_dim=5, hidden_nodes=[5, 10, 15], name='model'):\n",
    "#     '''\n",
    "#     How to call:\n",
    "#     myModel = create_keras_model(n_features, n_classes, hidden_nodes=[5, 10, 15], name='model')\n",
    "#     '''\n",
    "#     np.random.seed(42)\n",
    "#     tf.random.set_seed(42)\n",
    "\n",
    "#     # define input layer\n",
    "#     inputs = Input(shape=(input_dim,))\n",
    "\n",
    "#     # list to hold all the hidden layers\n",
    "#     hidden_layers = []\n",
    "\n",
    "#     # create the hidden layers\n",
    "#     for i in range(len(hidden_nodes)):\n",
    "#         if i == 0:\n",
    "#             # if it's the first hidden layer, connect it to the inputs\n",
    "#             hidden_layers.append(Dense(units=hidden_nodes[i], activation='relu', kernel_initializer='random_normal')(inputs))\n",
    "#         else:\n",
    "#             # otherwise, connect it to the previous hidden layer\n",
    "#             hidden_layers.append(Dense(units=hidden_nodes[i], activation='relu', kernel_initializer='random_normal')(hidden_layers[-1]))\n",
    "            \n",
    "#     # create the final hidden layer, connected to all previous hidden layers\n",
    "#     final_hidden = Dense(units=hidden_nodes[-1], activation='relu', kernel_initializer='random_normal')(concatenate(hidden_layers))\n",
    "\n",
    "#     # create output layer connected to the final hidden layer\n",
    "#     outputs = Dense(output_dim, activation='softmax')(final_hidden)\n",
    "\n",
    "#     # create model\n",
    "#     model = Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     return model\n",
    "\n",
    "# model0 = create_keras_model(n_features, n_classes, hidden_nodes=[6, 5, 5, 10], name='model')\n",
    "# model0.summary()\n",
    "# plot_model(model0, show_shapes=True)\n",
    "\n",
    "\n",
    "# COMPRSSED MODEL\n",
    "\n",
    "def create_keras_model(input_dim=11, output_dim=5, hidden_nodes=[[5, 2], [10, 4], [15, 8]]):\n",
    "    '''\n",
    "    How to call:\n",
    "    myModel = create_keras_model(n_features, n_classes, hidden_nodes=[[5, 2], [10, 4], [15, 8]])\n",
    "    '''\n",
    "\n",
    "    # define input layer\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "\n",
    "    # list to hold all the hidden layers\n",
    "    hidden_layers = []\n",
    "    compressed_layers = []\n",
    "\n",
    "    # create the hidden layers\n",
    "    for i in range(len(hidden_nodes)):\n",
    "        if i == 0:\n",
    "            # if it's the first hidden layer, connect it to the inputs\n",
    "            hidden_layers.append(Dense(units=hidden_nodes[i][0], activation='relu', kernel_initializer='random_normal')(inputs))\n",
    "        else:\n",
    "            # otherwise, connect it to the previous hidden layer\n",
    "            hidden_layers.append(Dense(units=hidden_nodes[i][0], activation='relu', kernel_initializer='random_normal')(hidden_layers[-1]))\n",
    "        \n",
    "        # create compression layer\n",
    "        compressed_layers.append(Dense(units=hidden_nodes[i][1], activation='relu', kernel_initializer='random_normal')(hidden_layers[-1]))\n",
    "\n",
    "    # create the final hidden layer, connected to all compression layers\n",
    "    final_hidden = Dense(units=hidden_nodes[-1][1], activation='relu', kernel_initializer='random_normal')(concatenate(compressed_layers))\n",
    "\n",
    "    # create output layer connected to the final hidden layer\n",
    "    outputs = Dense(output_dim, activation='softmax')(final_hidden)\n",
    "\n",
    "    # create model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model0 = create_keras_model(n_features, n_classes, hidden_nodes=[[6, 3], [5, 3], [5, 4]])\n",
    "model0.summary()\n",
    "plot_model(model0, show_shapes=True)\n",
    "\n",
    "model0firstfit = model0.fit(X_train, Y_train, batch_size=32, epochs=400, verbose=0, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def create_keras_model(input_dim=11, output_dim=5, hidden_nodes=[5, 10, 15], learning_rate=0.001, regularization_factor=0.01, early_stopping_monitor='val_loss', name='model'):\n",
    "    '''\n",
    "    How to call:\n",
    "    myModel = create_keras_model(n_features, n_classes, hidden_nodes=[5, 10, 15], learning_rate=0.001, regularization_factor=0.01, early_stopping_monitor='val_loss', name='model')\n",
    "    '''\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    # define input layer\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "\n",
    "    # list to hold all the hidden layers\n",
    "    hidden_layers = []\n",
    "\n",
    "    # create the hidden layers\n",
    "    for i in range(len(hidden_nodes)):\n",
    "        if i == 0:\n",
    "            # if it's the first hidden layer, connect it to the inputs\n",
    "            hidden_layers.append(Dense(units=hidden_nodes[i], activation='relu', kernel_initializer='random_normal', kernel_regularizer=l2(regularization_factor))(inputs))\n",
    "        else:\n",
    "            # otherwise, connect it to the previous hidden layer\n",
    "            hidden_layers.append(Dense(units=hidden_nodes[i], activation='relu', kernel_initializer='random_normal', kernel_regularizer=l2(regularization_factor))(hidden_layers[-1]))\n",
    "            \n",
    "    # create the final hidden layer, connected to all previous hidden layers\n",
    "    final_hidden = Dense(units=hidden_nodes[-1], activation='relu', kernel_initializer='random_normal', kernel_regularizer=l2(regularization_factor))(concatenate(hidden_layers))\n",
    "\n",
    "    # create output layer connected to the final hidden layer\n",
    "    outputs = Dense(output_dim, activation='softmax')(final_hidden)\n",
    "\n",
    "    # create model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "    # define optimizer with the given learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping callback\n",
    "    early_stopping_callback = EarlyStopping(monitor=early_stopping_monitor, patience=3)\n",
    "\n",
    "    return model, early_stopping_callback\n",
    "\n",
    "model0, early_stopping = create_keras_model(n_features, n_classes, hidden_nodes=[6, 5, 5, 10], learning_rate=0.001, regularization_factor=0.01, early_stopping_monitor='val_loss', name='model')\n",
    "model0.summary()\n",
    "plot_model(model0, show_shapes=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "model0firstfit = model0.fit(X_train, Y_train, validation_data=(X_test, Y_test), verbose=0, epochs=500, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(input_dim=11, output_dim=5, hidden_nodes=[[6, 3], [5, 3], [5, 4]], \n",
    "                       optimizer='adam', learning_rate=0.001, regularization_factor=0.01, \n",
    "                       early_stopping_monitor='val_loss', early_stop_patience=3, batch_size=32):\n",
    "    '''\n",
    "    How to call:\n",
    "    model_name = create_keras_model(n_features, n_classes, hidden_nodes=[[5, 2], [10, 4], [15, 8]], \n",
    "                                    optimizer='adam', learning_rate=0.001, regularization_factor=0.01, \n",
    "                                    early_stopping_monitor='val_loss', early_stopping_patience=3, batch_size=32)\n",
    "    '''\n",
    "\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    # input layer\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "\n",
    "    # lists to hold hidden layers\n",
    "    hidden_layers = []\n",
    "    compressed_layers = []\n",
    "\n",
    "    # create hidden layers\n",
    "    for i in range(len(hidden_nodes)):\n",
    "        if i == 0:\n",
    "            # if it's the first hidden layer, connect it to the inputs\n",
    "            hidden_layers.append(Dense(units=hidden_nodes[i][0], activation='relu', kernel_initializer='random_normal', kernel_regularizer=l2(regularization_factor))(inputs))\n",
    "        else:\n",
    "            # otherwise, connect it to the previous hidden layer\n",
    "            hidden_layers.append(Dense(units=hidden_nodes[i][0], activation='relu', kernel_initializer='random_normal', kernel_regularizer=l2(regularization_factor))(hidden_layers[-1]))\n",
    "        \n",
    "        # compression layer\n",
    "        compressed_layers.append(Dense(units=hidden_nodes[i][1], activation='relu', kernel_initializer='random_normal', kernel_regularizer=l2(regularization_factor))(hidden_layers[-1]))\n",
    "\n",
    "    # final hidden layer, connected to all compression layers\n",
    "    final_hidden = Dense(units=hidden_nodes[-1][1], activation='relu', kernel_initializer='random_normal', kernel_regularizer=l2(regularization_factor))(concatenate(compressed_layers))\n",
    "\n",
    "    # output layer, connected to final hidden layer\n",
    "    outputs = Dense(output_dim, activation='softmax')(final_hidden)\n",
    "\n",
    "    # create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # define optimizer with the given learning rate\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "    # Define early stopping callback\n",
    "    early_stopping_callback = EarlyStopping(monitor=early_stopping_monitor, patience=early_stop_patience)\n",
    "\n",
    "    return model, early_stopping_callback\n",
    "\n",
    "model0, early_stop = create_keras_model(n_features, n_classes, hidden_nodes=[[6, 3], [5, 3], [5, 4]], \n",
    "                                        optimizer='rmsprop', learning_rate=0.001, regularization_factor=0.01, \n",
    "                                        early_stopping_monitor='val_loss', early_stop_patience=100, batch_size=64)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working y Y ?i think yea no work\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Combine the one-hot encoded columns into a single target variable\n",
    "y = np.argmax(Y, axis=1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store evaluation results across folds\n",
    "accuracy_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train your model on the training set\n",
    "    # Define early stopping callback\n",
    "    early_stop0 = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    # Train the model\n",
    "    model0firstfit = model0.fit(X_train, y_train, validation_data=(X_test, y_test), verbose=1, batch_size=1, epochs=1000, callbacks=[early_stop0])\n",
    "    \n",
    "    # Test your model on the testing set\n",
    "    y_pred = model0firstfit.predict(X_test)\n",
    "    \n",
    "    # Convert the predicted probabilities to class labels\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Evaluate the model's performance on the testing set\n",
    "    accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Additional evaluation metrics or analysis\n",
    "    # ...\n",
    "    \n",
    "# Aggregate the evaluation results across folds\n",
    "mean_accuracy = np.mean(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# straight up first wrong \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store evaluation results across folds\n",
    "accuracy_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train your model on the training set\n",
    "    # Define early stopping callback\n",
    "    early_stop0 = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    # Train the model\n",
    "    model0firstfit = model0.fit(X_train, Y_train, validation_data=(X_test, Y_test), verbose=1, batch_size=1, epochs=1000, callbacks=[early_stop0])\n",
    "    \n",
    "    # Test your model on the testing set\n",
    "    y_pred = model0firstfit.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model's performance on the testing set\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Additional evaluation metrics or analysis\n",
    "    # ...\n",
    "    \n",
    "# Aggregate the evaluation results across folds\n",
    "mean_accuracy = np.mean(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accidental convergence to 4 nodes\n",
    "\n",
    "    # # create hidden layers\n",
    "    # for i in range(len(hidden_nodes)):\n",
    "    #     if i == 0:\n",
    "    #         # if it's the first hidden layer, connect it to the inputs\n",
    "    #         hidden_layers.append(Dense(units=hidden_nodes[i][0], activation='relu', kernel_initializer='random_normal', kernel_regularizer=l2(regularization_factor))(inputs))\n",
    "    #     else:\n",
    "    #         # otherwise, connect it to the previous hidden layer\n",
    "    #         hidden_layers.append(Dense(units=hidden_nodes[i][0], activation='relu', kernel_initializer='random_normal', kernel_regularizer=l2(regularization_factor))(hidden_layers[-1]))\n",
    "        \n",
    "    #     # compression layer\n",
    "    #     compressed_layers.append(Dense(units=hidden_nodes[i][1], activation='relu', kernel_initializer='random_normal', kernel_regularizer=l2(regularization_factor))(hidden_layers[-1]))\n",
    "\n",
    "    # # final hidden layer, connected to all compression layers\n",
    "    # final_hidden = Dense(units=hidden_nodes[-1][1], activation='relu', kernel_initializer='random_normal', kernel_regularizer=l2(regularization_factor))(concatenate(compressed_layers))\n",
    "\n",
    "    # # output layer, connected to final hidden layer\n",
    "    # outputs = Dense(output_dim, activation='softmax')(final_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping callback\n",
    "# early_stop1 = EarlyStopping(monitor='val_loss', patience=100)\n",
    "# Train the model\n",
    "# model1_fit = model1.fit(X_train, Y_train, validation_data=(X_test, Y_test), verbose=1, batch_size=1, epochs=1000, callbacks=[early_stop1])\n",
    "\n",
    "# maxes = []\n",
    "# for met in model0firstfit.history.keys():\n",
    "#     maxes.append(max(model0firstfit.history[met]))\n",
    "# mmax = max(maxes)\n",
    "\n",
    "# pd.DataFrame(model0firstfit.history).plot(figsize=(9, 5))\n",
    "# plt.grid(True)\n",
    "# plt.gca().set_ylim(0, 1.05*mmax)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap Keras model with KerasClassifier\n",
    "model_grid_search = KerasClassifier(build_fn=create_keras_model, verbose=0) #allows for create_keras_model to be called with additional parameters such as epochs and batch_size\n",
    "\n",
    "# set parameters\n",
    "early_stop_grid_search = EarlyStopping(monitor='val_loss', patience=50)\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64]\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "regularization_factors = [0.3, 0.25, 0.2, 0.15, 0.1, 0.5, 0.01, 0.05, 0.001, 0.0005]\n",
    "\n",
    "param_grid = dict(batch_size=batch_sizes, learning_rate=learning_rates, regularization_factor=regularization_factors)\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=5)\n",
    "grid_result = grid.fit(X_train, Y_train, callbacks=[early_stop_grid_search])\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from cv_results_\n",
    "df_grid_search = pd.DataFrame(grid_result.cv_results_)\n",
    "\n",
    "# pivot as prep for heatmap\n",
    "df_pivot = df_grid_search.pivot(index='param_batch_size', columns='param_epochs', values='mean_test_score')\n",
    "\n",
    "# create heatmap\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "sns.heatmap(df_pivot, annot=True, cmap=\"YlGnBu\")\n",
    "ax.set_title('Accuracy Heatmap')\n",
    "# ax.set_ylabel('Batch Size')\n",
    "# ax.set_xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "learning_rates = np.arange(0.001, 1.001, 0.001).tolist()\n",
    "len(learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap Keras model with KerasClassifier\n",
    "model_grid_search = KerasClassifier(build_fn=create_keras_model, verbose=1, batch_size=1)\n",
    "\n",
    "# set parameters\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "regularization_factors = [0.3, 0.25, 0.2, 0.15, 0.1, 0.5, 0.01, 0.05, 0.001, 0.0005]\n",
    "# learning_rates = [0.001, 0.01]\n",
    "# regularization_factors = [0.15, 0.1]\n",
    "\n",
    "param_grid = dict(learning_rate=learning_rates, regularization_factor=regularization_factors)\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model_grid_search, param_grid=param_grid, n_jobs=-1, cv=5)\n",
    "print(grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid.fit(X_train, Y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `grid` is your GridSearchCV object\n",
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "# Pivot the data for heatmap (Let's say we're using 'param_learning_rate' and 'param_regularization_factor')\n",
    "heatmap_data = cv_results.pivot(\n",
    "    index='param_learning_rate', \n",
    "    columns='param_regularization_factor', \n",
    "    values='mean_test_score'  # Change this if you want to plot something else\n",
    ")\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(heatmap_data, cmap='Blues',annot=True, fmt='.4f', cbar=False)\n",
    "\n",
    "plt.title('Hyperparameter Gridsearch Results')\n",
    "plt.xlabel('Regularization Factor')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # get predictions\n",
    "# y_pred = grid.predict(X_test)\n",
    "\n",
    "# # get accuracy\n",
    "# acc = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "# # print accuracy\n",
    "# print('Accuracy:', acc)\n",
    "\n",
    "# # print best parameters\n",
    "# print('Best Parameters:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing necessary modules\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "# from sklearn.preprocessing import label_binarize\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# from scipy import interp\n",
    "# from itertools import cycle\n",
    "# from sklearn.metrics import auc\n",
    "\n",
    "# # Remaining code is the same\n",
    "\n",
    "\n",
    "# # Making predictions\n",
    "# Y_train_pred_prob = model_prod.predict(X_train)\n",
    "# Y_test_pred_prob = model_prod.predict(X_test)\n",
    "\n",
    "# Y_train_pred = np.argmax(Y_train_pred_prob, axis=-1)\n",
    "# Y_test_pred = np.argmax(Y_test_pred_prob, axis=-1)\n",
    "\n",
    "# # If your Y_train and Y_test are already in one-hot encoded form (multilabel-indicator), \n",
    "# # convert them back to single label for confusion matrix and classification report\n",
    "# if len(Y_train.shape) > 1:\n",
    "#     Y_train_single_label = np.argmax(Y_train, axis=-1)\n",
    "#     Y_test_single_label = np.argmax(Y_test, axis=-1)\n",
    "# else:\n",
    "#     Y_train_single_label = Y_train\n",
    "#     Y_test_single_label = Y_test\n",
    "\n",
    "# # Printing confusion matrix\n",
    "# print(\"Training Confusion Matrix:\")\n",
    "# print(confusion_matrix(Y_train_single_label, Y_train_pred))\n",
    "\n",
    "# print(\"\\nTesting Confusion Matrix:\")\n",
    "# print(confusion_matrix(Y_test_single_label, Y_test_pred))\n",
    "\n",
    "# # Printing classification report\n",
    "# print(\"\\nTraining Classification Report:\")\n",
    "# print(classification_report(Y_train_single_label, Y_train_pred))\n",
    "\n",
    "# print(\"\\nTesting Classification Report:\")\n",
    "# print(classification_report(Y_test_single_label, Y_test_pred))\n",
    "\n",
    "# # Ensure binary format (only necessary if multiclass problem)\n",
    "# Y_train_bin = label_binarize(Y_train_single_label, classes=np.unique(Y_train_single_label))\n",
    "# Y_test_bin = label_binarize(Y_test_single_label, classes=np.unique(Y_test_single_label))\n",
    "\n",
    "# n_classes = Y_train_bin.shape[1]\n",
    "\n",
    "# # Compute ROC curve and ROC area for each class\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "# for i in range(n_classes):\n",
    "#     fpr[i], tpr[i], _ = roc_curve(Y_test_bin[:, i], Y_test_pred_prob[:, i])\n",
    "#     roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# # Plot ROC curves\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# lw = 2\n",
    "# colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "# for i, color in zip(range(n_classes), colors):\n",
    "#     plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "#              label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "#              ''.format(i, roc_auc[i]))\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic for multi-class data')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_of_results = {\n",
    "#     'Research Paper': ['Normalized Data','Stratified K Fold','97.95', '91.36'],\n",
    "#     'Ours'          : ['Normalized'     ,'K Fold',           f'{mean_accuracy_train_kf*100:.2f}' ,f'{mean_accuracy_test_kf*100:.2f}'],\n",
    "#     'Ours2'          : ['Normalized'  ,'Stratified K Fold',f'{mean_accuracy_train_skf*100:.2f}', f'{mean_accuracy_test_skf*100:.2f}'],\n",
    "#     'Ours3'          : ['Standardized','K Fold',           f'{mean_accuracy_train_kf*100:.2f}' ,f'{mean_accuracy_test_kf*100:.2f}'],\n",
    "#     'Ours4'          : ['Standardized','Stratified K Fold',f'{mean_accuracy_train_skf*100:.2f}', f'{mean_accuracy_test_skf*100:.2f}'],\n",
    "#     'Column5'    : ['','','Value9', 'Value10'],\n",
    "#     'Column6': ['','','Value11', 'Value12'],\n",
    "# }\n",
    "\n",
    "# print('Table of Results:')\n",
    "# dataframe_of_results = pd.DataFrame(table_of_results, index=['', '', 'Training', 'Testing'])\n",
    "# dataframe_of_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
